\section{Week 10: Analysis of Algorithm Efficiency}

\textit{Or: ``How long will this take, and why constants don't matter''}

\subsection*{Reading}
Epp \S 11.1--11.5.\\
\textbf{Category theory companion:} Week 10 (\texttt{category\_theory\_companion.pdf}).

\subsection*{Why Algorithm Analysis?}

You've written a program. It works. But is it \emph{fast}? Will it still work on inputs a thousand times larger? A million times? Understanding how running time grows with input size separates programs that scale from programs that don't.

Algorithm analysis gives us a language for discussing efficiency without getting bogged down in implementation details. When we say ``merge sort is $O(n \log n)$'' and ``bubble sort is $O(n^2)$,'' we're capturing something essential about these algorithms that holds regardless of the programming language, the compiler, or the hardware.

This week introduces asymptotic notation (big-$O$ and friends), techniques for analyzing loops and recursion, and the master theorem for solving divide-and-conquer recurrences. These tools are fundamental to thinking about algorithms.

\subsection*{Learning objectives}
\begin{itemize}
  \item Compare growth rates of functions using limits and dominance.
  \item Apply big-$O$, big-$\Omega$, and big-$\Theta$ notation correctly.
  \item Analyze the time complexity of loops and nested loops.
  \item Solve recurrences using expansion, substitution, and the master theorem.
  \item Classify algorithms by their complexity class.
\end{itemize}

\subsection*{Key definitions and facts}

\subsubsection*{Asymptotic Notation}

\begin{definition}[Asymptotic notation]
Let $f, g: \N \to \R^+$ be functions.

\textbf{Big-O (upper bound):} $f(n) = O(g(n))$ if there exist constants $c > 0$ and $n_0$ such that:
\[
f(n) \leq c \cdot g(n) \quad \text{for all } n \geq n_0
\]

\textbf{Big-Omega (lower bound):} $f(n) = \Omega(g(n))$ if there exist constants $c > 0$ and $n_0$ such that:
\[
f(n) \geq c \cdot g(n) \quad \text{for all } n \geq n_0
\]

\textbf{Big-Theta (tight bound):} $f(n) = \Theta(g(n))$ if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.
\end{definition}

The notation $f(n) = O(g(n))$ is a slight abuse---it's not really ``equals'' but ``is.'' Think of it as ``$f$ is big-O of $g$'' or ``$f$ grows no faster than $g$.''

Big-$O$ gives upper bounds. Big-$\Omega$ gives lower bounds. Big-$\Theta$ says the function is bounded both above and below---it captures the exact growth rate.

\begin{theorem}[Limit test]
If $\lim_{n \to \infty} \frac{f(n)}{g(n)} = L$, then:
\begin{itemize}
  \item $L = 0 \Rightarrow f(n) = O(g(n))$ but $f(n) \neq \Theta(g(n))$ (f grows slower)
  \item $0 < L < \infty \Rightarrow f(n) = \Theta(g(n))$ (same growth rate)
  \item $L = \infty \Rightarrow f(n) = \Omega(g(n))$ but $f(n) \neq O(g(n))$ (f grows faster)
\end{itemize}
\end{theorem}

\begin{definition}[Little-o and little-omega]
\textbf{Little-o:} $f(n) = o(g(n))$ means $\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0$. Strictly slower growth.

\textbf{Little-omega:} $f(n) = \omega(g(n))$ means $\lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty$. Strictly faster growth.
\end{definition}

\begin{theorem}[Properties of asymptotic notation]
\begin{enumerate}
  \item \textbf{Transitivity:} If $f = O(g)$ and $g = O(h)$, then $f = O(h)$.
  \item \textbf{Reflexivity:} $f = O(f)$, $f = \Omega(f)$, $f = \Theta(f)$.
  \item \textbf{Symmetry:} $f = \Theta(g)$ iff $g = \Theta(f)$.
  \item \textbf{Transpose symmetry:} $f = O(g)$ iff $g = \Omega(f)$.
  \item \textbf{Sum rule:} $O(f) + O(g) = O(\max(f, g))$.
  \item \textbf{Product rule:} $O(f) \cdot O(g) = O(f \cdot g)$.
  \item \textbf{Constant factors:} $O(cf) = O(f)$ for any constant $c > 0$.
\end{enumerate}
\end{theorem}

\subsection*{Common complexity classes}

\begin{definition}[Growth rate hierarchy]
From slowest to fastest:
\[
O(1) \subset O(\log n) \subset O(\sqrt{n}) \subset O(n) \subset O(n \log n) \subset O(n^2) \subset O(n^3) \subset O(2^n) \subset O(n!)
\]
\end{definition}

\begin{center}
\begin{tabular}{lll}
\textbf{Notation} & \textbf{Name} & \textbf{Example} \\
\hline
$O(1)$ & Constant & Array access \\
$O(\log n)$ & Logarithmic & Binary search \\
$O(n)$ & Linear & Linear search \\
$O(n \log n)$ & Linearithmic & Merge sort \\
$O(n^2)$ & Quadratic & Bubble sort \\
$O(n^3)$ & Cubic & Naive matrix multiplication \\
$O(2^n)$ & Exponential & Subset enumeration \\
$O(n!)$ & Factorial & Permutation enumeration
\end{tabular}
\end{center}

The jump from polynomial ($n^k$) to exponential ($2^n$) is critical. Polynomial-time algorithms are ``feasible''; exponential-time algorithms quickly become impractical.

\subsection*{Analyzing code}

\begin{theorem}[Loop analysis]
\begin{itemize}
  \item A loop running $n$ times with $O(1)$ body: $O(n)$
  \item Two nested loops, each running $n$ times: $O(n^2)$
  \item Three nested loops: $O(n^3)$
  \item A loop that halves the problem each iteration: $O(\log n)$
\end{itemize}
\end{theorem}

\begin{strategy}
To analyze a loop:
\begin{enumerate}
  \item Count iterations.
  \item Multiply by the cost of one iteration.
  \item For nested loops, multiply the iteration counts.
\end{enumerate}
\end{strategy}

\begin{theorem}[Summation formulas]
\begin{align*}
\sum_{i=1}^n 1 &= n \\
\sum_{i=1}^n i &= \frac{n(n+1)}{2} = \Theta(n^2) \\
\sum_{i=1}^n i^2 &= \frac{n(n+1)(2n+1)}{6} = \Theta(n^3) \\
\sum_{i=0}^n r^i &= \frac{r^{n+1} - 1}{r - 1} = \Theta(r^n) \text{ for } r > 1 \\
\sum_{i=1}^n \frac{1}{i} &= \Theta(\log n) \text{ (harmonic series)}
\end{align*}
\end{theorem}

\subsection*{Recurrence relations}

\begin{definition}[Recurrence relation]
A \textbf{recurrence relation} expresses $T(n)$ in terms of smaller inputs. Common divide-and-conquer form:
\[
T(n) = aT(n/b) + f(n)
\]
where $a \geq 1$ subproblems of size $n/b$ are solved, and $f(n)$ is the work outside recursion.
\end{definition}

\begin{theorem}[Master theorem]
For $T(n) = aT(n/b) + f(n)$ with $a \geq 1$, $b > 1$:

Let $c = \log_b a$. Compare $f(n)$ with $n^c$:

\textbf{Case 1:} If $f(n) = O(n^{c - \epsilon})$ for some $\epsilon > 0$, then $T(n) = \Theta(n^c)$.
(Recursion dominates.)

\textbf{Case 2:} If $f(n) = \Theta(n^c \log^k n)$ for some $k \geq 0$, then $T(n) = \Theta(n^c \log^{k+1} n)$.
(Balanced.)

\textbf{Case 3:} If $f(n) = \Omega(n^{c + \epsilon})$ for some $\epsilon > 0$ and $af(n/b) \leq kf(n)$ for some $k < 1$, then $T(n) = \Theta(f(n))$.
(Work dominates.)
\end{theorem}

\begin{theorem}[Common recurrences]
\begin{center}
\begin{tabular}{lll}
\textbf{Recurrence} & \textbf{Solution} & \textbf{Example} \\
\hline
$T(n) = T(n/2) + O(1)$ & $O(\log n)$ & Binary search \\
$T(n) = T(n-1) + O(1)$ & $O(n)$ & Linear recursion \\
$T(n) = T(n-1) + O(n)$ & $O(n^2)$ & Selection sort \\
$T(n) = 2T(n/2) + O(1)$ & $O(n)$ & Tree traversal \\
$T(n) = 2T(n/2) + O(n)$ & $O(n \log n)$ & Merge sort \\
$T(n) = 2T(n-1) + O(1)$ & $O(2^n)$ & Naive Fibonacci
\end{tabular}
\end{center}
\end{theorem}

\subsection*{Solving recurrences}

\begin{strategy}
\textbf{Method 1: Expansion (iteration)}
\begin{enumerate}
  \item Expand the recurrence several times.
  \item Identify the pattern.
  \item Sum the terms.
\end{enumerate}

\textbf{Method 2: Substitution (guess and verify)}
\begin{enumerate}
  \item Guess the form of the solution.
  \item Use induction to verify.
  \item Adjust constants as needed.
\end{enumerate}

\textbf{Method 3: Master theorem}
\begin{enumerate}
  \item Identify $a$, $b$, and $f(n)$.
  \item Compute $c = \log_b a$.
  \item Determine which case applies.
\end{enumerate}
\end{strategy}

\subsection*{Best, worst, and average case}

\begin{definition}[Case analysis]
\begin{itemize}
  \item \textbf{Worst case:} Maximum time over all inputs of size $n$.
  \item \textbf{Best case:} Minimum time over all inputs of size $n$.
  \item \textbf{Average case:} Expected time over a probability distribution on inputs.
\end{itemize}
Usually, we report worst-case complexity---it provides guarantees.
\end{definition}

\subsection*{Complexity classes and reductions}

\begin{definition}[P and NP]
\begin{itemize}
  \item $\mathbf{P}$: Problems solvable in polynomial time by a deterministic algorithm.
  \item $\mathbf{NP}$: Problems whose solutions can be \emph{verified} in polynomial time. Equivalently, solvable in polynomial time by a nondeterministic algorithm.
\end{itemize}
\end{definition}

$\mathbf{P} \subseteq \mathbf{NP}$ (anything solvable quickly is certainly verifiable quickly). Whether $\mathbf{P} = \mathbf{NP}$ is the most famous open problem in computer science.

\begin{definition}[Reductions and NP-completeness]
A polynomial-time reduction from $A$ to $B$ (written $A \leq_p B$) transforms instances of $A$ into instances of $B$ preserving yes/no answers.
\begin{itemize}
  \item $B$ is \textbf{NP-hard} if every problem in NP reduces to $B$.
  \item $B$ is \textbf{NP-complete} if $B \in \mathbf{NP}$ and $B$ is NP-hard.
\end{itemize}
\end{definition}

If any NP-complete problem is in $\mathbf{P}$, then $\mathbf{P} = \mathbf{NP}$. The prevailing belief is that this doesn't happen.

\begin{strategy}
To prove a problem $B$ is NP-complete:
\begin{enumerate}
  \item Show $B \in \mathbf{NP}$ (solutions are verifiable in polynomial time).
  \item Reduce a known NP-complete problem $A$ to $B$ in polynomial time.
\end{enumerate}
\end{strategy}

\begin{example}[Common NP-complete problems]
SAT, 3-SAT, CLIQUE, VERTEX COVER, HAMILTONIAN CYCLE, SUBSET SUM.
\end{example}

\subsection*{Worked examples}

\begin{example}[Proving big-Theta]
Show that $3n^2 + 5n + 7 = \Theta(n^2)$.

\emph{Solution.}

\textbf{Upper bound:} For $n \geq 1$:
\[
3n^2 + 5n + 7 \leq 3n^2 + 5n^2 + 7n^2 = 15n^2
\]
So $3n^2 + 5n + 7 = O(n^2)$ with $c = 15$, $n_0 = 1$.

\textbf{Lower bound:} For $n \geq 1$:
\[
3n^2 + 5n + 7 \geq 3n^2
\]
So $3n^2 + 5n + 7 = \Omega(n^2)$ with $c = 3$, $n_0 = 1$.

Therefore $3n^2 + 5n + 7 = \Theta(n^2)$.
\end{example}

\begin{example}[Ordering by growth rate]
Order: $n \log n$, $n^{1.5}$, $2^n$, $n^3$.

\emph{Solution.} Use limits:
\begin{itemize}
  \item $\lim \frac{n \log n}{n^{1.5}} = \lim \frac{\log n}{\sqrt{n}} = 0$ (L'Hôpital). So $n \log n = o(n^{1.5})$.
  \item $\lim \frac{n^{1.5}}{n^3} = \lim \frac{1}{n^{1.5}} = 0$. So $n^{1.5} = o(n^3)$.
  \item $\lim \frac{n^3}{2^n} = 0$ (exponential dominates). So $n^3 = o(2^n)$.
\end{itemize}

Order: $n \log n \prec n^{1.5} \prec n^3 \prec 2^n$.
\end{example}

\begin{example}[Nested loops, both to $n$]
Analyze:
\begin{verbatim}
for i = 1 to n:
    for j = 1 to n:
        // O(1) operation
\end{verbatim}

\emph{Solution.} Inner loop runs $n$ times per outer iteration. Outer runs $n$ times. Total: $n \times n = n^2$. Complexity: $O(n^2)$.
\end{example}

\begin{example}[Nested loops, triangular]
Analyze:
\begin{verbatim}
for i = 1 to n:
    for j = 1 to i:
        // O(1) operation
\end{verbatim}

\emph{Solution.} Inner loop runs $i$ times. Total iterations:
\[
\sum_{i=1}^n i = \frac{n(n+1)}{2} = \Theta(n^2)
\]
\end{example}

\begin{example}[Master theorem]
Solve $T(n) = 2T(n/2) + n$ with $T(1) = 1$.

\emph{Solution.} Identify: $a = 2$, $b = 2$, $f(n) = n$.

$c = \log_b a = \log_2 2 = 1$, so $n^c = n$.

Compare: $f(n) = n = \Theta(n^1) = \Theta(n^c \log^0 n)$.

This is Case 2 with $k = 0$: $T(n) = \Theta(n^c \log^{k+1} n) = \Theta(n \log n)$.
\end{example}

\begin{example}[Expansion method]
Solve $T(n) = 2T(n/2) + n$ by expansion.

\emph{Solution.}
\begin{align*}
T(n) &= 2T(n/2) + n \\
     &= 2[2T(n/4) + n/2] + n = 4T(n/4) + 2n \\
     &= 4[2T(n/8) + n/4] + 2n = 8T(n/8) + 3n \\
     &\vdots \\
     &= 2^k T(n/2^k) + kn
\end{align*}

When $n/2^k = 1$, we have $k = \log_2 n$ and $T(1) = 1$:
\[
T(n) = 2^{\log n} \cdot 1 + n \log n = n + n \log n = \Theta(n \log n)
\]
\end{example}

\begin{example}[Binary search]
Analyze binary search.

\emph{Solution.} Each step halves the search space. Recurrence:
\[
T(n) = T(n/2) + O(1), \quad T(1) = O(1)
\]
By master theorem: $a = 1$, $b = 2$, $f(n) = O(1)$. $c = \log_2 1 = 0$, so $n^c = 1$.

$f(n) = \Theta(n^0)$. Case 2 with $k = 0$: $T(n) = \Theta(\log n)$.
\end{example}

\begin{example}[Log factorial]
Show that $\log(n!) = \Theta(n \log n)$.

\emph{Solution.}

\textbf{Upper bound:} $n! \leq n^n$, so $\log(n!) \leq n \log n$.

\textbf{Lower bound:} $n! \geq (n/2)^{n/2}$ (considering only the top half of terms), so:
\[
\log(n!) \geq \frac{n}{2} \log \frac{n}{2} = \frac{n}{2}(\log n - 1) = \Omega(n \log n)
\]

Therefore $\log(n!) = \Theta(n \log n)$.
\end{example}

\begin{example}[Log grows slower than any polynomial]
Prove $\log n = O(n^\epsilon)$ for any $\epsilon > 0$.

\emph{Proof.} Compute the limit:
\[
\lim_{n \to \infty} \frac{\log n}{n^\epsilon}
\]
This is $\infty/\infty$. Apply L'Hôpital:
\[
\lim_{n \to \infty} \frac{1/n}{\epsilon n^{\epsilon - 1}} = \lim_{n \to \infty} \frac{1}{\epsilon n^\epsilon} = 0
\]
Since the limit is 0, $\log n = O(n^\epsilon)$. In fact, $\log n = o(n^\epsilon)$: log grows strictly slower than any positive power.
\end{example}

\begin{example}[Constant factors in exponents matter]
Show $2^{n+1} = \Theta(2^n)$ but $2^{2n} \neq O(2^n)$.

\emph{Solution.}

\textbf{Part 1:} $2^{n+1} = 2 \cdot 2^n$. So $2^{n+1} = \Theta(2^n)$ with constant 2.

\textbf{Part 2:} $2^{2n} = (2^2)^n = 4^n$.
\[
\lim_{n \to \infty} \frac{4^n}{2^n} = \lim_{n \to \infty} 2^n = \infty
\]
So $4^n$ grows faster than $2^n$, meaning $2^{2n} \neq O(2^n)$.

\textbf{Key insight:} Adding a constant to the exponent is fine; multiplying the exponent is not.
\end{example}

\begin{example}[Master theorem, Case 1]
Solve $T(n) = 3T(n/2) + n$.

\emph{Solution.} $a = 3$, $b = 2$, $f(n) = n$.

$c = \log_2 3 \approx 1.585$.

Compare $f(n) = n = n^1$ with $n^c = n^{1.585}$. Since $1 < 1.585$, we have $f(n) = O(n^{c - \epsilon})$ for $\epsilon = 0.585$.

Case 1: $T(n) = \Theta(n^c) = \Theta(n^{\log_2 3})$.
\end{example}

\begin{example}[Linear recursion]
Solve $T(n) = T(n-1) + n$ by expansion.

\emph{Solution.}
\begin{align*}
T(n) &= T(n-1) + n \\
     &= T(n-2) + (n-1) + n \\
     &= T(n-3) + (n-2) + (n-1) + n \\
     &\vdots \\
     &= T(1) + 2 + 3 + \cdots + n \\
     &= T(1) + \frac{n(n+1)}{2} - 1
\end{align*}
If $T(1) = 1$: $T(n) = \frac{n(n+1)}{2} = \Theta(n^2)$.
\end{example}

\begin{commonmistake}
\textbf{Treating big-O as equality.} $f = O(g)$ means $f$ is bounded above by $g$, not equal. ``$f$ is $O(g)$'' is more accurate than ``$f$ equals $O(g)$.''
\end{commonmistake}

\begin{commonmistake}
\textbf{Confusing worst-case and big-O.} Big-O describes function growth. Worst-case describes inputs. They're related but distinct: worst-case running time \emph{is} a function that can be described with big-O.
\end{commonmistake}

\begin{commonmistake}
\textbf{Forgetting the regularity condition in Case 3.} The master theorem's Case 3 requires $af(n/b) \leq kf(n)$ for some $k < 1$. Usually satisfied, but verify.
\end{commonmistake}

\begin{goingdeeper}[Going Deeper: Monoids---Categories with One Object]
Throughout this course, we've seen how categories unify mathematics. We end with a beautiful observation: \emph{monoids are categories with exactly one object}.
For more detail, see the Category Theory Companion, Week 10.

\subsubsection*{Monoids: The Definition}

A \textbf{monoid} $(M, \cdot, e)$ is a set $M$ with:
\begin{itemize}
  \item A binary operation $\cdot : M \times M \to M$
  \item An identity element $e \in M$
\end{itemize}
satisfying:
\begin{itemize}
  \item \textbf{Associativity:} $(a \cdot b) \cdot c = a \cdot (b \cdot c)$
  \item \textbf{Identity:} $e \cdot a = a = a \cdot e$
\end{itemize}

Compare to category axioms:
\[
\begin{array}{c|c}
\textbf{Monoid} & \textbf{Category} \\
\hline
\text{Elements of } M & \text{Morphisms} \\
\text{Multiplication } \cdot & \text{Composition } \circ \\
\text{Identity } e & \text{Identity morphism } \id \\
\text{Associativity} & \text{Associativity}
\end{array}
\]

If a category has only one object $\star$, all morphisms go from $\star$ to $\star$---they can always be composed. The morphisms form a monoid.

\subsubsection*{Examples of Monoids}

\begin{enumerate}
  \item $(\N, +, 0)$: natural numbers under addition.
  \item $(\N, \times, 1)$: natural numbers under multiplication.
  \item $(\Sigma^*, \cdot, \varepsilon)$: strings under concatenation (the \emph{free monoid} on $\Sigma$).
  \item $(A \to A, \circ, \id_A)$: functions from $A$ to itself under composition.
\end{enumerate}

\subsubsection*{The Free Monoid}

Given alphabet $\Sigma$, the \textbf{free monoid} $\Sigma^*$ is all strings with concatenation. It's ``free'' because it satisfies no equations beyond the monoid axioms.

\textbf{Universal property:} For any monoid $M$ and function $f: \Sigma \to M$, there's a unique monoid homomorphism $\bar{f}: \Sigma^* \to M$ extending $f$.

This is why $\Sigma^*$ appears in automata theory: a DFA computes a monoid homomorphism from $\Sigma^*$ to a finite monoid.

\subsubsection*{Monoid Homomorphisms}

A \textbf{monoid homomorphism} $\phi: M \to N$ satisfies:
\begin{itemize}
  \item $\phi(a \cdot b) = \phi(a) \cdot \phi(b)$
  \item $\phi(e_M) = e_N$
\end{itemize}

In categorical terms: a homomorphism between one-object categories is exactly a functor.

\textbf{Example:} String length $\text{len}: \Sigma^* \to \N$ is a homomorphism: $\text{len}(s \cdot t) = \text{len}(s) + \text{len}(t)$ and $\text{len}(\varepsilon) = 0$.

\subsubsection*{Connection to Complexity Analysis}

Consider the monoid $(\N, +, 0)$ of running times. When we analyze:
\[
T(n) = T(n/2) + O(1)
\]
we're adding times (monoid operation) of recursive work plus local work.

For divide-and-conquer with $a$ subproblems:
\[
T(n) = \underbrace{T(n/b) + T(n/b) + \cdots + T(n/b)}_{a \text{ terms}} + f(n)
\]
The structure of recurrences reflects monoid structure.

\subsubsection*{The Categorical Perspective: Full Circle}

We began with sets and functions, introduced diagrams, and discovered that many structures---preorders, graphs, automata---are categorical. Now we see that even monoids are secretly categories.

This suggests a powerful principle: \emph{category theory reveals common structure across diverse areas}. Types, proofs, and programs all form categories. The tools we've developed---diagrams, universal properties, functors---are the beginning of a vocabulary for understanding computation, logic, and mathematics as aspects of a unified whole.

\subsubsection*{Exercises: Monoids and Structure}

\begin{enumerate}
  \item Verify that $(\Z, +, 0)$ is a monoid. Is $(\Z, -, 0)$ a monoid? Why or why not?

  \item The set $\{0, 1\}$ with OR ($\lor$) and identity 0 forms a monoid. Write its multiplication table.

  \item Show that $n \times n$ matrices under multiplication with identity $I$ form a monoid.

  \item Is string length a monoid homomorphism from $(\Sigma^*, \cdot, \varepsilon)$ to $(\N, +, 0)$? Prove it.

  \item Prove: If $\phi: M \to N$ is a monoid homomorphism, then $\phi(M)$ is a submonoid of $N$.

  \item Consider $(\Z_n, +_n, 0)$ where $+_n$ is addition mod $n$. Prove $\phi(k) = k \mod n$ is a homomorphism from $\Z$.

  \item \textbf{Challenge:} $\mathcal{P}(A)$ forms a monoid under union (identity $\emptyset$) and under intersection (identity $A$). Are these monoids isomorphic?

  \item Define big-$O$ equivalence: $f \sim g$ iff $f = \Theta(g)$. Show the equivalence classes form a monoid under multiplication.
\end{enumerate}
\end{goingdeeper}

\subsection*{Practice}
\begin{enumerate}
  \item Order: $n \log n$, $n^{1.5}$, $2^n$, $n^3$ by growth rate.

  \item Show that $3n^2 + 5n + 7$ is $\Theta(n^2)$.

  \item Solve $T(n) = 2T(n/2) + n$ with $T(1) = 1$.

  \item Analyze the runtime of binary search.

  \item Prove: $\log n = O(n^\epsilon)$ for any $\epsilon > 0$.

  \item Analyze:
\begin{verbatim}
for i = 1 to n:
    for j = i to n:
        for k = 1 to j:
            // O(1)
\end{verbatim}

  \item Solve $T(n) = 3T(n/2) + n$ using the master theorem.

  \item Prove: $(n+1)! = O((n!)^2)$ but $(n+1)! \neq \Theta(n!)$.

  \item Analyze $T(n) = T(n-1) + n$ with $T(1) = 1$.

  \item Show $2^{n+1} = \Theta(2^n)$ but $2^{2n} \neq \Theta(2^n)$.

  \item Prove: $f(n) = o(g(n))$ implies $f(n) = O(g(n))$.

  \item A recursive algorithm satisfies $T(n) = T(n/3) + T(2n/3) + n$. Prove $T(n) = O(n \log n)$.

  \item If $A \leq_p B$ and $B \in \mathbf{P}$, what can you conclude about $A$?

  \item Explain why HAMILTONIAN CYCLE is in $\mathbf{NP}$.

  \item Outline a reduction from CLIQUE to VERTEX COVER.
\end{enumerate}
