\section{Week 6: Expected Value and Introduction to Graphs}

\textit{Or: ``What happens on average, and how things connect''}

\subsection*{Reading}
Epp \S 9.8; 10.1; 10.2.\\
\textbf{Category theory companion:} Weeks 6--7 (\texttt{category\_theory\_companion.pdf}).

\subsection*{Why Expected Value?}

When outcomes are random, you can't predict what will happen, but you can often say something useful about what happens \emph{on average}. Expected value captures this: it's the long-run average you'd see if you repeated an experiment many times.

Expected value is central to decision-making under uncertainty, algorithm analysis (average-case complexity), and basically anywhere probability meets the real world. The key insight this week is \emph{linearity of expectation}---a shockingly powerful tool that makes many calculations trivial.

\subsection*{Why Graphs?}

Graphs are everywhere: social networks, road maps, the internet, molecular structures, dependency relationships in software. Anything with ``things'' and ``connections between things'' is a graph.

This week introduces the basic vocabulary: vertices, edges, degrees, adjacency. The handshake theorem---the sum of degrees equals twice the number of edges---is our first fundamental result about graph structure.

\subsection*{Learning objectives}
\begin{itemize}
  \item Define and compute expected value for discrete random variables.
  \item Apply linearity of expectation to simplify calculations.
  \item Use indicator random variables for counting.
  \item Define graphs, vertices, edges, and basic terminology.
  \item Apply the handshake theorem to relate degrees and edges.
  \item Distinguish simple graphs, multigraphs, and digraphs.
\end{itemize}

\subsection*{Part I: Expected Value}

\subsubsection*{Probability Foundations}

\begin{definition}[Probability axioms]
A \textbf{probability measure} on a sample space $S$ assigns to each event $A \subseteq S$ a number $P(A)$ satisfying:
\begin{enumerate}
  \item $P(A) \geq 0$ for all events $A$ (non-negativity)
  \item $P(S) = 1$ (something happens)
  \item If $A_1, A_2, \ldots$ are pairwise disjoint, then $P(\bigcup_i A_i) = \sum_i P(A_i)$ (additivity)
\end{enumerate}
\end{definition}

From these three axioms, everything else follows: $P(\emptyset) = 0$, $P(A^c) = 1 - P(A)$, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, etc.

\begin{definition}[Random variable]
A \textbf{random variable} $X$ on sample space $S$ is a function $X: S \to \R$ assigning a real number to each outcome. For discrete random variables, the possible values form a finite or countable set.
\end{definition}

\begin{definition}[Expected value]
The \textbf{expected value} (or \textbf{expectation} or \textbf{mean}) of a discrete random variable $X$ is:
\[
E[X] = \sum_x x \cdot P(X = x)
\]
where the sum is over all possible values $x$ of $X$.
\end{definition}

Think of expected value as a weighted average: each value is weighted by its probability.

\begin{theorem}[Linearity of expectation]
For any random variables $X$ and $Y$ and constants $a, b$:
\[
E[aX + bY] = aE[X] + bE[Y]
\]
More generally:
\[
E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i]
\]
\end{theorem}

\begin{keyresult}
Linearity of expectation works \emph{regardless of whether the random variables are independent}. This is its superpower. You can break a complicated random variable into simple pieces, compute expectations of the pieces separately, and add them up---even when the pieces are tangled together.
\end{keyresult}

\begin{definition}[Indicator random variable]
The \textbf{indicator} $I_A$ for event $A$ is:
\[
I_A = \begin{cases} 1 & \text{if } A \text{ occurs} \\ 0 & \text{otherwise} \end{cases}
\]
Key fact: $E[I_A] = P(A)$.
\end{definition}

Indicators are the workhorse of expected value calculations. If you're counting ``how many of events $A_1, \ldots, A_n$ occur,'' write $X = I_{A_1} + \cdots + I_{A_n}$ and apply linearity:
\[
E[X] = P(A_1) + \cdots + P(A_n)
\]

\begin{definition}[Common distributions]
\begin{itemize}
  \item \textbf{Bernoulli($p$):} $X = 1$ with probability $p$, $X = 0$ with probability $1-p$. $E[X] = p$.
  \item \textbf{Binomial($n, p$):} Number of successes in $n$ independent trials. $E[X] = np$.
  \item \textbf{Geometric($p$):} Number of trials until first success. $E[X] = 1/p$.
  \item \textbf{Uniform on $\{1, \ldots, n\}$:} Each value equally likely. $E[X] = (n+1)/2$.
\end{itemize}
\end{definition}

\begin{definition}[Variance]
The \textbf{variance} of $X$ measures spread around the mean:
\[
\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2
\]
Standard deviation is $\sigma = \sqrt{\text{Var}(X)}$.
\end{definition}

\subsection*{Part II: Introduction to Graphs}

\begin{definition}[Graph]
A \textbf{graph} $G = (V, E)$ consists of:
\begin{itemize}
  \item $V$: a finite nonempty set of \textbf{vertices} (or nodes)
  \item $E$: a set of \textbf{edges}, each connecting two vertices
\end{itemize}
\end{definition}

\begin{definition}[Types of graphs]
\begin{itemize}
  \item \textbf{Simple graph:} No loops (edges from a vertex to itself) and no multiple edges.
  \item \textbf{Multigraph:} Allows multiple edges between the same pair.
  \item \textbf{Pseudograph:} Allows loops and multiple edges.
  \item \textbf{Directed graph (digraph):} Edges have direction.
\end{itemize}
Unless otherwise stated, ``graph'' means simple graph.
\end{definition}

\begin{definition}[Basic terminology]
\begin{itemize}
  \item Two vertices are \textbf{adjacent} if an edge connects them.
  \item An edge is \textbf{incident} to its endpoints.
  \item The \textbf{degree} $\deg(v)$ is the number of edges incident to $v$ (loops count twice).
  \item Degree 0 = \textbf{isolated}; degree 1 = \textbf{leaf} (or pendant).
  \item The \textbf{neighborhood} $N(v)$ is the set of vertices adjacent to $v$.
\end{itemize}
\end{definition}

\begin{theorem}[Handshake theorem]
In any graph $G = (V, E)$:
\[
\sum_{v \in V} \deg(v) = 2|E|
\]
\end{theorem}

\emph{Why it works:} Each edge has two endpoints. When we sum degrees, each edge gets counted exactly twice (once at each end). \qed

\begin{corollary}
Every graph has an even number of vertices with odd degree.
\end{corollary}

\emph{Proof:} The sum of degrees is even (it's $2|E|$). If you add up numbers and get an even total, you must have an even number of odd summands. \qed

\begin{definition}[Special graphs]
\begin{itemize}
  \item \textbf{Complete graph $K_n$:} All $\binom{n}{2}$ possible edges present.
  \item \textbf{Cycle $C_n$:} Vertices $v_1, \ldots, v_n$ with edges $v_1v_2, v_2v_3, \ldots, v_{n-1}v_n, v_nv_1$.
  \item \textbf{Path $P_n$:} Like a cycle but no edge from $v_n$ to $v_1$; has $n-1$ edges.
  \item \textbf{Complete bipartite $K_{m,n}$:} Two parts of sizes $m$ and $n$; every cross-pair connected; $mn$ edges.
  \item \textbf{$n$-cube $Q_n$:} Vertices are $n$-bit strings; edges connect strings differing in one bit.
\end{itemize}
\end{definition}

\begin{definition}[Degree sequence]
The \textbf{degree sequence} is the list of degrees in non-increasing order. Example: $K_4$ has degree sequence $(3, 3, 3, 3)$.
\end{definition}

A natural question: given a sequence of numbers, is it the degree sequence of some graph? The handshake theorem gives a necessary condition (sum must be even), but that's not sufficient. The Erd≈ës--Gallai conditions or Havel--Hakimi algorithm give complete answers.

\begin{definition}[Subgraph and complement]
$H$ is a \textbf{subgraph} of $G$ if $H$'s vertices and edges are subsets of $G$'s.

The \textbf{complement} $\overline{G}$ has the same vertices as $G$; two vertices are adjacent in $\overline{G}$ iff they're not adjacent in $G$.
\end{definition}

\begin{goingdeeper}[Category Lens: Graphs as a Category]
Graphs form a category $\cat{Graph}$ where morphisms are \textbf{graph homomorphisms}---vertex maps that preserve adjacency. If $\phi: G \to H$ is a homomorphism and $uv$ is an edge in $G$, then $\phi(u)\phi(v)$ is an edge in $H$.

There's a forgetful functor $U: \cat{Graph} \to \Set$ sending a graph to its vertex set. This lets us use set-theoretic reasoning while respecting graph structure.

Events (subsets of a sample space) correspond to characteristic functions $A \to 2$. If $f: A \to B$, the preimage map $f^{-1}: \Pow(B) \to \Pow(A)$ is contravariant and preserves unions/intersections---explaining why probability behaves nicely under functions.
\end{goingdeeper}

\subsection*{Worked examples}

\begin{example}[Die roll]
A fair die is rolled. Let $X$ be the outcome. Compute $E[X]$.

\emph{Solution.} Each outcome 1--6 has probability $1/6$:
\[
E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \cdots + 6 \cdot \frac{1}{6} = \frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5
\]
\end{example}

\begin{example}[Coin flips with linearity]
A fair coin is flipped 10 times. What is the expected number of heads?

\emph{Solution.} Let $X_i = 1$ if flip $i$ is heads, 0 otherwise. Then $X = X_1 + \cdots + X_{10}$ counts heads.

By linearity: $E[X] = E[X_1] + \cdots + E[X_{10}] = 10 \cdot \frac{1}{2} = 5$.

Note: we didn't need to know the distribution of $X$ explicitly!
\end{example}

\begin{example}[Fixed points in permutations]
In a random permutation of $\{1, \ldots, n\}$, what is the expected number of fixed points?

\emph{Solution.} Let $X_i = 1$ if element $i$ is in position $i$. Then $X = \sum_{i=1}^n X_i$.

$P(\text{element } i \text{ fixed}) = 1/n$ (out of $n!$ permutations, $(n-1)!$ fix position $i$).

By linearity: $E[X] = n \cdot \frac{1}{n} = 1$.

The expected number of fixed points is exactly 1, regardless of $n$. This is a beautiful result!
\end{example}

\begin{example}[Geometric distribution]
What is the expected number of die rolls to get a 6?

\emph{Solution.} This is geometric with success probability $p = 1/6$.
\[
E[X] = \frac{1}{p} = 6
\]
\end{example}

\begin{example}[Handshake theorem for $K_4$]
Verify the handshake theorem for the complete graph $K_4$.

\emph{Solution.} $K_4$ has 4 vertices, each with degree 3.
\begin{itemize}
  \item Sum of degrees: $3 + 3 + 3 + 3 = 12$
  \item Number of edges: $\binom{4}{2} = 6$
  \item Check: $2 \times 6 = 12$ \checkmark
\end{itemize}
\end{example}

\begin{example}[Degree sequence realizability]
Is there a simple graph with degree sequence $(3, 3, 2, 2, 2)$?

\emph{Solution.} Sum = 12, which is even. \checkmark

Use Havel--Hakimi: take the largest degree (3), remove it, subtract 1 from the next 3 degrees:
\[
(3, 3, 2, 2, 2) \to (2, 1, 1, 2) \to (2, 2, 1, 1)
\]
Repeat: $(2, 2, 1, 1) \to (1, 0, 1) \to (1, 1, 0) \to (0, 0)$ \checkmark

Yes, such a graph exists.
\end{example}

\begin{example}[Edges in $Q_n$]
How many edges does the $n$-cube $Q_n$ have?

\emph{Solution.} $Q_n$ has $2^n$ vertices (all $n$-bit strings). Each vertex has degree $n$ (flip any of $n$ bits).

Sum of degrees: $n \cdot 2^n$.

By handshake: $|E| = \frac{n \cdot 2^n}{2} = n \cdot 2^{n-1}$.
\end{example}

\begin{example}[Pigeonhole for degrees]
Prove: Every simple graph on $n \geq 2$ vertices has at least two vertices of the same degree.

\emph{Solution.} In a simple graph, degrees range from 0 to $n-1$ (can't have degree $n$---no loops). That's $n$ possible values.

But wait: if some vertex has degree 0 (isolated), no vertex can have degree $n-1$ (connected to everyone). So at most $n-1$ distinct degrees are achievable.

By pigeonhole: $n$ vertices, at most $n-1$ distinct degrees $\Rightarrow$ two vertices share a degree.
\end{example}

\begin{commonmistake}
\textbf{Assuming independence for linearity.} The formula $E[X + Y] = E[X] + E[Y]$ works even when $X$ and $Y$ are dependent! Many students add ``assuming independence'' when it's not needed.
\end{commonmistake}

\begin{commonmistake}
\textbf{Confusing $E[XY]$ with $E[X] \cdot E[Y]$.} These are equal only when $X$ and $Y$ are independent. In general, $E[XY] \neq E[X]E[Y]$.
\end{commonmistake}

\subsection*{Practice}
\begin{enumerate}
  \item A coin is flipped 10 times. What is the expected number of heads?

  \item Show that the sum of degrees in a tree on $n$ vertices is $2(n-1)$.

  \item Find $E[X]$ for a geometric random variable with success probability $p$.

  \item Decide whether a graph with degree sequence $(3, 3, 2, 2, 2)$ is possible.

  \item In a random permutation of $\{1, \ldots, n\}$, what is the expected number of elements greater than all previous elements?

  \item How many edges does $K_{4,5}$ have? What are the vertex degrees?

  \item Prove that the complement of $K_n$ is an empty graph.

  \item A bag has 5 red and 3 blue marbles. Two are drawn without replacement. What is the expected number of red marbles drawn?

  \item Show that a simple graph on $n$ vertices has at most $\binom{n}{2}$ edges.

  \item Using handshake: If every vertex has degree $\geq k$, show $|E| \geq k|V|/2$.

  \item Prove every graph has an even number of odd-degree vertices.

  \item In a room of 100 people, everyone shakes hands with exactly 3 others. Is this possible?
\end{enumerate}
